{
  
    
        "post0": {
            "title": "An Essentials Guide to PyTorch Dataset and DataLoader Usage",
            "content": "Overview . In this short guide, we show a small representative example using the Dataset and DataLoader classes available in PyTorch for easy batching of training examples. This is more meant to be an onboarding for me with fastpages, but hopefully this example will be useful to those beginning to use PyTorch for their own applications. . Setup . The first thing we need is the essential import: torch, i.e. PyTorch. Make sure that when you&#39;re running a notebook with code similar to this that you&#39;ve imported torch, i.e. import torch, as shown below. . #collapse_hide import torch . . We&#39;ll then need a dataset to work with. For this small example, we&#39;ll use numpy to generate a random dataset for us. Specifically, we&#39;ll be working with a batch size of 32 later, so we&#39;ll create a dataset with exactly 50 batches, where each example has 5 features and a corresponding label between 0-9, inclusive. To do so, we use . np.random.randn for generating the input examples | np.random.randint for generating the labels | . The exact code is shown below. . #collapse_show import numpy as np training_examples = np.random.randn(32 * 50, 5) training_labels = np.random.randint(0, 10, size=(32*50,)) . . As a sanity check, let&#39;s look at the shapes. We&#39;ll want the size of the whole dataset to be (1600, 5), as we have $32*50$ examples, each with 5 features. Similarly, we&#39;ll want the size of the labels for the whole dataset to be (1600,), as we&#39;re essentially working with a list of 1600 labels. . #collapse_show training_examples.shape, training_labels.shape . . ((1600, 5), (1600,)) . We can look at some of the labels, just for a sanity check that they look reasonable. . #collapse_show training_labels[:10] . . array([8, 4, 3, 3, 9, 8, 1, 0, 5, 8]) . Dataset Class and Instantiation . Now, we&#39;ll create a simple PyTorch dataset class. All you need to implement within this class is the __getitem__ function and the __len__ function. . __getitem__ is a function that takes in an index, and returns dataset[index] | __len__ returns the size of your dataset (in this case, that&#39;s 32*50). | . When writing this class, you MUST subclass torch.utils.data.Dataset, as this is requirement for using the DataLoader class (see below). . class ExampleDataset(torch.utils.data.Dataset): &quot;&quot;&quot; You can define the __init__ function any way you like&quot;&quot;&quot; def __init__(self, examples, labels): self.examples = examples self.labels = labels &quot;&quot;&quot; This function signature always should take in 1 argument, corresponding to the index you&#39;re going to access. In this case, we&#39;re returning a tuple, corresponding to the training example and the corresponding label. It will also be useful to convert the returned values to torch.Tensors, so we can push the data onto the GPU later on. Note how the label is put into an array, but the example isn&#39;t. This is just a convention: if we don&#39;t put `self.labels[index]` in a list, it&#39;ll just create a tensor of zeros with `self.labels[index]` zeros. &quot;&quot;&quot; def __getitem__(self, index): return (torch.Tensor(self.examples[index]), torch.Tensor([self.labels[index]])) &quot;&quot;&quot; This function signature always should take in 0 arguments, and the output should be an `int`. &quot;&quot;&quot; def __len__(self): return len(self.examples) . Now, we can instantiate an instance of our ExampleDataset class, which subclasses torch.utils.data.Dataset. Note that we can specify how to initialize this via the __init__ function, which takes in a list of examples, and a list of labels (i.e. what we&#39;ve instantiated in our own setup). . training_dataset = ExampleDataset(training_examples, training_labels) . Sanity check - see the correspondence between accessing the dataset instance of the class above and the examples/labels we passed in. . training_dataset[0] . (tensor([ 0.4257, 0.0499, 0.2127, 0.8207, -1.1252]), tensor([8.])) . training_examples[0], training_labels[0] . (array([ 0.42565871, 0.04990118, 0.21268054, 0.82069534, -1.12520862]), 8) . We can iterate over this dataset using standard for loop syntax as well. The way you write the for loop depends on how __getitem__ is set up. In our case, we return a tuple (example and label), so the for loop should also have a tuple. . example, label = training_dataset[0] print(type(example), example.shape, type(label), label.shape) . &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([5]) &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([1]) . from tqdm import tqdm for example, label in tqdm(training_dataset): continue . 100%|██████████| 1600/1600 [00:00&lt;00:00, 85500.98it/s] . Batching via the DataLoader class . To set up batching, we&#39;ll use the torch.utils.data.DataLoader class. All we have to do to create this DataLoader is to instantiate it with our dataset we created above (training_dataset). The arguments for torch.utils.data.DataLoader are worth looking at, but (generally) most important are: . dataset: the PyTorch dataset class instance we&#39;ll pass in (e.g. training_dataset, this is why we had to do subclassing above) | batch_size (optional, default is 1): the batch size we want when iterating (we&#39;ll pass in 32) | shuffle (optional, default is False): whether we want to shuffle when iterating once the dataloader (note that if this is set to true, it&#39;ll shuffle every epoch; note also that we only really want to have this set to true for training, not necessarily for validation) | drop_last (optional, default is False): whether to drop the last incomplete batch (we don&#39;t have to worry about this because the number of training examples is exactly divisible by 32) | . training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True) . Again, we can iterate, just like we did for training_dataset, but now, we get batches back, as we can see by printing the shapes. The magic happens in the collate_fn optional argument of the DataLoader class, but the default behavior is sufficient here for batching the examples and labels separately. . We&#39;ll first ensure that there are exactly 50 batches in our dataloader to work with. . assert len(training_dataloader) == 50 . Now, mimicking the iteration from above, with the ExampleDataset instance: . for example, label in tqdm(training_dataloader): continue . 100%|██████████| 50/50 [00:00&lt;00:00, 2255.73it/s] . At some point, you may want to know information about a specific batch - accessing specific batches from the DataLoader is not as easy - I don&#39;t know of a way to grab a specific batch, other than doing something like the following. . training_dataloader_batches = [(example, label) for example, label in training_dataloader] . some_example, some_label = training_dataloader_batches[15] some_example.shape, some_label.shape . (torch.Size([32, 5]), torch.Size([32, 1])) . However, you can always access the underlying dataset by literally doing .dataset, as shown below. . training_dataloader.dataset . &lt;__main__.ExampleDataset at 0x7f2f03857510&gt; . training_dataloader.dataset[15] . (tensor([ 0.0763, -0.3285, -1.0820, 0.2322, -0.2848]), tensor([8.])) . GPU Usage . Using the GPU is also trivial, even with the batches from the dataloader. Ensure that you have the GPU runtime set first, then run the following. You can verify that GPU is available with the condition shown below before the loop. . if torch.cuda.is_available(): print(&quot;Using GPU.&quot;) . Using GPU. . for example, label in tqdm(training_dataloader): if torch.cuda.is_available(): example, label = example.cuda(), label.cuda() . 100%|██████████| 50/50 [00:03&lt;00:00, 15.79it/s] . Afterword and Resources . As mentioned above, it&#39;s useful to look at the documentation for torch.utils.data.DataLoader. Another way to do so within the notebook itself is to run the following within a cell of the notebook: . torch.utils.data.DataLoader? . There are many interesting things that you can do here, with the arguments allowed in the DataLoader. For example: . You may be working with an image dataset large enough that you don&#39;t want to open all the images (e.g. using PIL) before feeding them through your model. In that case, you can lazily open them by passing in a collate_fn that opens the images before collating the examples of a batch, since collate_fn is only called for each iteration when iterating over the DataLoader, and not when the DataLoader is instantiated. | You may not want to shuffle the dataset, as it might incur unnecessary computation. This is especially true if you have a separate DataLoader for your validation dataset, in which case there&#39;s no need to shuffle, as it won&#39;t affect the predictions. | If you have access to multiple CPUs on whatever machine you&#39;re working on, you can use num_workers to load batches ahead of time on the other CPUs, i.e. the other workers. | If you&#39;re working with a GPU, one of the more expensive steps in the pipeline is moving data from CPU to GPU - this can be sped up by using pin_memory, which ensures that the same space in the GPU RAM is used for the data being transferred from the CPU. | .",
            "url": "https://dthiagarajan.github.io/technical_blog/pytorch/2020/03/09/PyTorch-Dataset-and-DataLoaders.html",
            "relUrl": "/pytorch/2020/03/09/PyTorch-Dataset-and-DataLoaders.html",
            "date": " • Mar 9, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://dthiagarajan.github.io/technical_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://dthiagarajan.github.io/technical_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://dthiagarajan.github.io/technical_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}