{
  
    
        "post0": {
            "title": "Basic Image Classification with the PCam Dataset [Draft]",
            "content": "In this post, we&#39;ll take a look at the PatchCamelyon benchmark dataset found on Kaggle. In particular, from the description on Kaggle: . [PCam] packs the clinically-relevant task of metastasis detection into a straight-forward binary image classification task, akin to CIFAR-10 and MNIST. Models can easily be trained on a single GPU in a couple hours, and achieve competitive scores in the Camelyon16 tasks of tumor detection and whole-slide image diagnosis. Furthermore, the balance between task-difficulty and tractability makes it a prime suspect for fundamental machine learning research on topics as active learning, model uncertainty, and explainability. . The aspects that will be noteworthy to inspect in this post are as follows: Comparing how well a pre-trained network works compared to training from scratch on this dataset Using weights from training runs in this notebook later on for fine-tuning on more complex tasks, e.g. localization or segmentation of histopathological slides, or for training on whole-slide images (WSIs). . We&#39;ll also take a look at typical evaluation metrics - in this case, we&#39;ll look at the area under the ROC curve, and show how we might be able to optimize more closely for that metric, rather than just accuracy alone. . Below are all imports necessary for the notebook. . #collapse_hide import csv import matplotlib.pyplot as plt import numpy as np import os from PIL import Image import tifffile as tiff import torch import torch.nn as nn import torch.optim as optim from torch.nn import functional as F import torch.utils.model_zoo as model_zoo from torchvision import transforms from tqdm.notebook import tqdm . . Dataset . Before we got to the model code, we&#39;ll first look at the dataset. We&#39;ll load it from file, and look at label distributions to account for any label bias before doing train/validation splits (the testing examples are already given to us, but we should only evaluate on them once). . First, let&#39;s look at the directory structure. . pcam_directory = &#39;/Users/dilip.thiagarajan/data/pcam&#39; . !ls $pcam_directory . sample_submission.csv train test train_labels.csv . !ls $pcam_directory/train | head -n 10 . 00001b2b5609af42ab0ab276dd4cd41c3e7745b5.tif 000020de2aa6193f4c160e398a8edea95b1da598.tif 00004aab08381d25d315384d646f5ce413ea24b1.tif 0000d563d5cfafc4e68acb7c9829258a298d9b6a.tif 0000da768d06b879e5754c43e2298ce48726f722.tif 0000f8a4da4c286eee5cf1b0d2ab82f979989f7b.tif 00010f78ea8f878117500c445a658e5857f4e304.tif 00011545a495817817c6943583b294c900a137b8.tif 000126ec42770c7568204e2f6e07eb9a07d5e121.tif 00014e39b5df5f80df56f18a0a049d1cc6de430a.tif . !head -n 10 $pcam_directory/train_labels.csv . id,label f38a6374c348f90b587e046aac6079959adf3835,0 c18f2d887b7ae4f6742ee445113fa1aef383ed77,1 755db6279dae599ebb4d39a9123cce439965282d,0 bc3f0c64fb968ff4a8bd33af6971ecae77c75e08,0 068aba587a4950175d04c680d38943fd488d6a9d,0 acfe80838488fae3c89bd21ade75be5c34e66be7,0 a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da,1 7f6ccae485af121e0b6ee733022e226ee6b0c65f,1 559e55a64c9ba828f700e948f6886f4cea919261,0 . label_mapping = {} with open(os.path.join(pcam_directory, &#39;train_labels.csv&#39;), &#39;r&#39;) as f: reader = csv.reader(f) next(reader) # To skip the header label_mapping = {slide_id: int(label) for [slide_id, label] in reader} . With our labels read in, we can look at the label distribution. From below, we can see that the split is about 60/40 in terms of benign vs. malignant slides. This is good for the problem being standalone because: . If we use a network that was pre-trained on a curated dataset (i.e. a dataset with balanced distribution of images within each class), we don&#39;t need to worry about how the network assumes no bias. | We don&#39;t need to weight our loss objective towards a given class too much to account for the difference here (10% to being unbiased). | . However, this might not be good downstream, as: . Most problems in medical imaging of binary classification deal with very unbalanced datasets, so for more complex problems (e.g. segmentation, where &gt;80% of pixels are background), we&#39;ll have to account for that more carefully. | . freq = {} for _, label in label_mapping.items(): if label not in freq: freq[label] = 0. freq[label] += 1. freq = {k: v / len(label_mapping) for (k, v) in freq.items()} freq, len(label_mapping) . ({0: 0.5949687535507329, 1: 0.40503124644926713}, 220025) . In any case, we&#39;ll proceed. We&#39;ll split without any other nuance for our train/validation split. In particular, we&#39;ll just organize the filepaths to the various images (in this case, .tif files), and not open the images immediately - this will be done during training and evaluation. The reason for this is primarily to avoid having to open all images and keep them in RAM. . all_fps = [fp for fp in os.listdir(os.path.join(pcam_directory, &#39;train&#39;))] . We can quickly verify that all files in this list are .tif files. . for fp in all_fps: assert fp[-4:] == &#39;.tif&#39;, fp[-4:] . Now, we&#39;ll split using a random permutation of the indices. We&#39;ll use (approximately) an 80/20 train/validation split. We can also include the labels here for convenience. . permutation = np.random.permutation(range(len(all_fps))) train_fps, val_fps = ( [ (os.path.join(pcam_directory, &#39;train&#39;, all_fps[index]), label_mapping[all_fps[index][:-4]]) for index in permutation[:int(len(permutation) * .8)] ], [ (os.path.join(pcam_directory, &#39;train&#39;, all_fps[index]), label_mapping[all_fps[index][:-4]]) for index in permutation[int(len(permutation) * .8):] ] ) . len(train_fps), len(val_fps) . (176020, 44005) . train_fps[:5] . [(&#39;/Users/dilip.thiagarajan/data/pcam/train/36b24336a581a8f61f5d6967e04358a55621eebe.tif&#39;, 0), (&#39;/Users/dilip.thiagarajan/data/pcam/train/da621acc52824617ed70a088c4cb64c7d8d27e80.tif&#39;, 1), (&#39;/Users/dilip.thiagarajan/data/pcam/train/86a29bc10de2dde63accd98c89f4c9cf77ae6a53.tif&#39;, 0), (&#39;/Users/dilip.thiagarajan/data/pcam/train/e5a9ff18ec439f498183f00067da67034d9b25de.tif&#39;, 0), (&#39;/Users/dilip.thiagarajan/data/pcam/train/75b7bd31ff7b07a6f4ebe462e44d0e98e5b67c51.tif&#39;, 0)] . Let&#39;s take a look at one of these images. . example_image_fp, example_label = train_fps[0] example_image = Image.open(example_image_fp) print(&quot;Size of image&quot;, example_image.size) plt.imshow(example_image) . Size of image (96, 96) . &lt;matplotlib.image.AxesImage at 0x1446e9350&gt; . We can take a look at the unique image sizes in our dataset. From the Kaggle data description, our goal is as follows: . You are predicting the labels for the images in the test folder. A positive label indicates that the center 32x32px region of a patch contains at least one pixel of tumor tissue. Tumor tissue in the outer region of the patch does not influence the label. This outer region is provided to enable fully-convolutional models that do not use zero-padding, to ensure consistent behavior when applied to a whole-slide image. . sizes = {} for fp, _ in tqdm(train_fps): sizes[Image.open(fp).size] = 1 for fp, _ in tqdm(val_fps): sizes[Image.open(fp).size] = 1 . . sizes . {(96, 96): 1} . So, we know that we don&#39;t need to reshape in any particular way. Any random image transformations we&#39;ll perform will maintain this shape. . Now, we&#39;ll set up our PyTorch Dataset class (for a brief introduction, see my previous post). We&#39;ll include image transforms shortly, so we&#39;ll set up that argument as well here. . class PCamDataset(torch.utils.data.Dataset): def __init__(self, examples, transform=None): self.examples = examples self.transform = transform def __getitem__(self, index): image_fp, label = self.examples[index] image = Image.open(image_fp) if self.transform is not None: image = self.transform(image) return image, torch.Tensor([label]).long() def __len__(self): return len(self.examples) . Here are the transforms we use. As mentioned before: . All random transforms will maintain the shape of the image | We&#39;ll first train using the pre-trained network, and show how that compares to training with a network from scratch. Since we&#39;re using a pre-trained network, we&#39;ll use the statistics for normalization from that dataset (ImageNet). | Validation transforms don&#39;t include any random transforms (though they could be used in addition to the additional image to get an ensemble prediction, i.e. to perform test-time augmentation for prediction). | . train_transforms = transforms.Compose([ transforms.ColorJitter(), transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Imagenet statistics ]) val_transforms = transforms.Compose([ transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Imagenet statistics ]) . Now, we&#39;ll build our dataset using the class we defined above, and accordingly get our dataloaders. . tr_ds, val_ds = PCamDataset(train_fps, transform=train_transforms), PCamDataset(val_fps, transform=val_transforms) tr_dl, val_dl = ( torch.utils.data.DataLoader(tr_ds, batch_size=32, shuffle=True), torch.utils.data.DataLoader(val_ds, batch_size=32) ) . Now that we&#39;ve built our dataset, we&#39;ll implement the classification network. For this, we&#39;ll use ResNets, pre-trained on ImageNet. First, we&#39;ll set up some helper functions. . #collapse_hide model_urls = { &#39;resnet18&#39;: &#39;https://download.pytorch.org/models/resnet18-5c106cde.pth&#39;, &#39;resnet34&#39;: &#39;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#39;, &#39;resnet50&#39;: &#39;https://download.pytorch.org/models/resnet50-19c8e357.pth&#39;, &#39;resnet101&#39;: &#39;https://download.pytorch.org/models/resnet101-5d3b4d8f.pth&#39;, &#39;resnet152&#39;: &#39;https://download.pytorch.org/models/resnet152-b121ed2d.pth&#39;, } def conv3x3(in_planes, out_planes, stride=1): &quot;&quot;&quot;Return a 3x3 convolution taking in `in_planes` filters and outputting `out_planes` filters with padding and stride `stride`.&quot;&quot;&quot; return nn.Conv2d( in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False ) def conv1x1(in_planes, out_planes, stride=1): &quot;&quot;&quot;Return a 1x1 convolution taking in `in_planes` filters and outputting `out_planes` filters with padding and stride `stride`.&quot;&quot;&quot; return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False) . . Then, we&#39;ll set up the blocks for the ResNet. . #collapse_hide class BasicBlock(nn.Module): &quot;&quot;&quot;The basic building block for ResNets, encompassing the residual connection. Takes in `inplanes` number of input filters, outputs `planes` number of output filters, with stride `stride` on the first 3x3 conv, and an optional downsampling (`nn.Module`) on the residual output via the `downsample` parameter.&quot;&quot;&quot; expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out class Bottleneck(nn.Module): &quot;&quot;&quot;The bottleneck building block for ResNets, encompassing the residual connection. Takes in `inplanes` number of input filters, outputs `planes` number of output filters, with stride `stride` on the first 3x3 conv, and an optional downsampling (`nn.Module`) on the residual output via the `downsample` parameter.&quot;&quot;&quot; expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None): super(Bottleneck, self).__init__() self.conv1 = conv1x1(inplanes, planes) self.bn1 = nn.BatchNorm2d(planes) self.conv2 = conv3x3(planes, planes, stride) self.bn2 = nn.BatchNorm2d(planes) self.conv3 = conv1x1(planes, planes * self.expansion) self.bn3 = nn.BatchNorm2d(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out . . And finally, the ResNet class itself, and functions to instantiate the networks with pre-trained weights. In particular, since the pre-trained weights are meant for ImageNet, but we&#39;re classifying to two classes, we modify the architecture so that the last layer is an FC trained from scratch, and will not have pre-trained weights. The rest of the network will still get pre-trained weights. . #collapse_hide class ResNet(nn.Module): &quot;&quot;&quot;Constructs a ResNet using a block specification and number of layers. Also specifies where to output training logs and model/system state.&quot;&quot;&quot; def __init__( self, block, layers, num_classes=1000): super(ResNet, self).__init__() self.inplanes = 64 self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2) self.layer3 = self._make_layer(block, 256, layers[2], stride=2) self.layer4 = self._make_layer(block, 512, layers[3], stride=2) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.out_fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;relu&#39;) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) def _make_layer(self, block, planes, blocks, stride=1): downsample = None if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), nn.BatchNorm2d(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample)) self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append(block(self.inplanes, planes)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) x = x.view(x.size(0), -1) x = self.out_fc(x) return x def resnet18(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-18 model.&quot;&quot;&quot; model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet18&#39;]), strict=False) return model def resnet34(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-34 model.&quot;&quot;&quot; model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet34&#39;]), strict=False) return model def resnet50(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-50 model.&quot;&quot;&quot; model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet50&#39;]), strict=False) return model def resnet101(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-101 model.&quot;&quot;&quot; model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet101&#39;]), strict=False) return model def resnet152(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-152 model.&quot;&quot;&quot; model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet152&#39;]), strict=False) return model . . Next, we&#39;ll set up the model, loss function objective, optimizer, and learning rate schedule (in this case, we use the OneCycle learning rate scheduler). We&#39;ll start by just training the output FC&#39;s parameters. . model = resnet18(pretrained=True, num_classes=2) if torch.cuda.is_available(): model = model.cuda() optimizer = torch.optim.AdamW([{&#39;params&#39;: model.out_fc.parameters(), &#39;lr&#39;: 1e-3}]) criterion = nn.CrossEntropyLoss() num_epochs = 10 scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=1e-2, total_steps=num_epochs * len(tr_dl), ) . Now, we can train. We&#39;ll set up a general training function to repeatedly call for each epoch. . def train(model, epoch, dataloader, criterion, optimizer, scheduler=None): model.train() total, total_loss, total_correct = 0, 0., 0. tqdm_iterator = tqdm(enumerate(dataloader), total=len(dataloader)) for i, (x, y) in tqdm_iterator: if torch.cuda.is_available(): x, y = x.cuda(), y.cuda() optimizer.zero_grad() output = model(x) prediction = torch.argmax(output, -1) loss = criterion(output, y.squeeze()) total_loss += loss.item() * len(y) total_correct += (prediction == y.squeeze()).sum().item() total += len(y) loss.backward() optimizer.step() if scheduler is not None: scheduler.step() tqdm_iterator.set_postfix({ &quot;Loss&quot;: (total_loss / total), &quot;Accuracy&quot;: (total_correct / total) }) final_loss, final_acc = total_loss / total, total_correct / total log.info( &quot;Reporting %.5f training loss, %.5f training accuracy for epoch %d.&quot; % (final_loss, final_acc, epoch) ) return final_loss, final_acc . for i in range(num_epochs): train(model, i + 1, tr_dl, criterion, optimizer, scheduler) . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-130-3fb1e608c6e2&gt; in &lt;module&gt; 1 for i in range(num_epochs): -&gt; 2 train(model, i + 1, tr_dl, criterion, optimizer, scheduler) &lt;ipython-input-129-29395fad0c99&gt; in train(model, epoch, dataloader, criterion, optimizer, scheduler) 8 x, y = x.cuda(), y.cuda() 9 optimizer.zero_grad() &gt; 10 output = model(x) 11 prediction = torch.argmax(output, -1) 12 loss = criterion(output, y.squeeze()) ~/anaconda3/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) &lt;ipython-input-100-d82805ff9d17&gt; in forward(self, x) 48 x = self.maxpool(x) 49 &gt; 50 x = self.layer1(x) 51 x = self.layer2(x) 52 x = self.layer3(x) ~/anaconda3/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) ~/anaconda3/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/container.py in forward(self, input) 98 def forward(self, input): 99 for module in self: --&gt; 100 input = module(input) 101 return input 102 ~/anaconda3/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) &lt;ipython-input-99-21d9e3510037&gt; in forward(self, x) 20 residual = x 21 &gt; 22 out = self.conv1(x) 23 out = self.bn1(out) 24 out = self.relu(out) ~/anaconda3/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) ~/anaconda3/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/conv.py in forward(self, input) 343 344 def forward(self, input): --&gt; 345 return self.conv2d_forward(input, self.weight) 346 347 class Conv3d(_ConvNd): ~/anaconda3/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/conv.py in conv2d_forward(self, input, weight) 340 _pair(0), self.dilation, self.groups) 341 return F.conv2d(input, weight, self.bias, self.stride, --&gt; 342 self.padding, self.dilation, self.groups) 343 344 def forward(self, input): KeyboardInterrupt: . Next, we&#39;ll train on the full network. We&#39;ll use a smaller learning rate for the backbone of the network, and slightly larger learning rate for the output FC of the network. We&#39;ll also vary the learning rate logarithmically, such that the initial part of the network has the lowest learning rate, and as we move right, the learning rate increases. . base_lr = 1e-3 lower_bound_factor = 1e-5 upper_bound_factor = 1e-2 body_parameters = [ (param, _) for (param, _) in model.named_parameters() if param.split(&#39;.&#39;)[0] != &#39;out_fc&#39; ] lrs = np.geomspace( base_lr * lower_bound_factor, base_lr * upper_bound_factor, len(body_parameters) ) param_lr_maps = [ {&#39;params&#39;: param, &#39;lr&#39;: lr} for ((_, param), lr) in zip(body_parameters, lrs) ] param_lr_maps.append({&#39;params&#39;: model.out_fc.parameters(), &#39;lr&#39;: base_lr}) . optimizer = torch.optim.AdamW(param_lr_maps, lr=base_lr) criterion = nn.CrossEntropyLoss() num_epochs = 20 scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=base_lr, total_steps=num_epochs * len(tr_dl), ) . Now, we can finetune the whole network. . for i in range(num_epochs): train(model, i + 1, tr_dl, criterion, optimizer, scheduler) . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-133-3fb1e608c6e2&gt; in &lt;module&gt; 1 for i in range(num_epochs): -&gt; 2 train(model, i + 1, tr_dl, criterion, optimizer, scheduler) &lt;ipython-input-129-29395fad0c99&gt; in train(model, epoch, dataloader, criterion, optimizer, scheduler) 8 x, y = x.cuda(), y.cuda() 9 optimizer.zero_grad() &gt; 10 output = model(x) 11 prediction = torch.argmax(output, -1) 12 loss = criterion(output, y.squeeze()) ~/anaconda3/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) &lt;ipython-input-100-d82805ff9d17&gt; in forward(self, x) 46 x = self.bn1(x) 47 x = self.relu(x) &gt; 48 x = self.maxpool(x) 49 50 x = self.layer1(x) ~/anaconda3/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) ~/anaconda3/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/pooling.py in forward(self, input) 139 return F.max_pool2d(input, self.kernel_size, self.stride, 140 self.padding, self.dilation, self.ceil_mode, --&gt; 141 self.return_indices) 142 143 ~/anaconda3/envs/technical_blog/lib/python3.7/site-packages/torch/_jit_internal.py in fn(*args, **kwargs) 179 return if_true(*args, **kwargs) 180 else: --&gt; 181 return if_false(*args, **kwargs) 182 183 if if_true.__doc__ is None and if_false.__doc__ is not None: ~/anaconda3/envs/technical_blog/lib/python3.7/site-packages/torch/nn/functional.py in _max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices) 486 stride = torch.jit.annotate(List[int], []) 487 return torch.max_pool2d( --&gt; 488 input, kernel_size, stride, padding, dilation, ceil_mode) 489 490 max_pool2d = boolean_dispatch( KeyboardInterrupt: . Now, we can evaluate the network. . Note: this post is a draft. I still need to add the code for evaluation, as well as some further additions, including visualizing gradient-weighted class-activation maps. .",
            "url": "https://dthiagarajan.github.io/technical_blog/draft/pytorch/medical%20imaging/2020/03/31/Basic-Image-Classification-with-the-PCam-Dataset.html",
            "relUrl": "/draft/pytorch/medical%20imaging/2020/03/31/Basic-Image-Classification-with-the-PCam-Dataset.html",
            "date": " • Mar 31, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Using NBDev to Package Dynamic U-Net Code [Draft]",
            "content": "In the last post, we saw how to build the dynamic U-Net architecture using pre-trained ResNets as the encoder backbone. In this post, we can show how to use that code base without having to copy paste, thanks to NBDev. . First, make sure to install the package, which can be found here. . Note: For installing packages, I like to use the command $(which pip) install, to ensure I&#8217;m installing to the right site packages directory. Then, we import it below. . #show import dynamic_unet . We&#39;ll use some finer-grained imports to mimic exactly what we did in the previous blog post. . #show from dynamic_unet.encoder import resnet34 from dynamic_unet.unet import DynamicUNet from dynamic_unet.utils import CamvidDataset, display_segmentation_from_file, load_camvid_dataset from dynamic_unet.opt import dice_similarity, DiceLoss import torch from tqdm.notebook import tqdm . Next, let&#39;s load the dataset. . #collapse_show camvid_data_directory = &quot;/home/jupyter/data/camvid&quot; all_data, val_indices, label_mapping = load_camvid_dataset(camvid_data_directory); . . Split it into train/validation splits. . #collapse_show tr_data, val_data = [tpl for i, tpl in enumerate(all_data) if i not in val_indices], [tpl for i, tpl in enumerate(all_data) if i in val_indices] . . Let&#39;s visualize an example quickly. . i = 10 display_segmentation_from_file(tr_data[i][0], tr_data[i][1]) . Now, let&#39;s initialize the PyTorch dataset and dataloaders. . tr_ds, val_ds = CamvidDataset(tr_data, resize_shape=(360, 480)), CamvidDataset(val_data, resize_shape=(360, 480), is_train=False) tr_dl, val_dl = torch.utils.data.DataLoader(tr_ds, batch_size=4, shuffle=True), torch.utils.data.DataLoader(val_ds) . Finally, we&#39;ll initialize our model, loss, and optimizer. For now, we&#39;ll just look to train the decoder parameters. More details can be found in the previous post for further fine-tuning. . model = DynamicUNet(resnet34(), num_output_channels=32, input_size=(360, 480)) if torch.cuda.is_available(): model = model.cuda() decoder_parameters = [item for module in model.decoder for item in module.parameters()] optimizer = torch.optim.AdamW(decoder_parameters) # Only training the decoder for now criterion = DiceLoss() # Training specific parameters num_epochs = 10 num_up_epochs, num_down_epochs = 3, 7 scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=1e-2, total_steps=num_epochs * len(tr_dl), ) . We&#39;ll copy over the training loop from the previous post to see how the model trains. . #collapse_hide model.train() losses = [] accuracies = [] tqdm_iterator = tqdm(range(num_epochs), position=0) for epoch in tqdm_iterator: tr_loss, tr_correct_pixels, tr_total_pixels, tr_dice_similarity, total = 0., 0., 0., 0., 0. tqdm_epoch_iterator = tqdm(tr_dl, position=1, leave=False) for i, (x, y) in enumerate(tqdm_epoch_iterator): optimizer.zero_grad() if torch.cuda.is_available(): x, y = x.cuda(), y.squeeze(dim=1).cuda() output = model(x) probs = torch.softmax(output, dim=1) prediction = torch.argmax(output, dim=1) tr_correct_pixels += ((prediction == y).sum()) tr_total_pixels += y.numel() tr_dice_similarity += dice_similarity(output, y.squeeze(1)) * len(y) loss = criterion(output, y.squeeze(1)) tr_loss += loss.data.cpu() * len(y) total += len(y) loss.backward() optimizer.step() if scheduler is not None: scheduler.step() if i % 1 == 0: curr_loss = tr_loss / total curr_acc = tr_correct_pixels / tr_total_pixels curr_dice = tr_dice_similarity / total tqdm_epoch_iterator.set_postfix({ &quot;Loss&quot;: curr_loss.item(), &quot;Accuracy&quot;: curr_acc.item(), &quot;Dice&quot;: curr_dice.item() }) overall_loss = tr_loss.item() / total overall_acc = tr_correct_pixels.item() / tr_total_pixels losses.append(overall_loss) accuracies.append(overall_acc) tqdm_iterator.set_postfix({&quot;Loss&quot;: overall_loss, &quot;Accuracy&quot;: overall_acc}) . . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-9-4caa77be1f34&gt; in &lt;module&gt; 12 if torch.cuda.is_available(): 13 x, y = x.cuda(), y.squeeze(dim=1).cuda() &gt; 14 output = model(x) 15 probs = torch.softmax(output, dim=1) 16 prediction = torch.argmax(output, dim=1) /opt/conda/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) /opt/conda/envs/technical_blog/lib/python3.7/site-packages/dynamic_unet/unet.py in forward(self, x) 35 36 try: &gt; 37 self.encoder(x) 38 finally: 39 if self.verbose &gt;= 1: /opt/conda/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) /opt/conda/envs/technical_blog/lib/python3.7/site-packages/dynamic_unet/encoder.py in forward(self, x) 146 x = self.layer2(x) 147 x = self.layer3(x) --&gt; 148 x = self.layer4(x) 149 150 return x /opt/conda/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) /opt/conda/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/container.py in forward(self, input) 98 def forward(self, input): 99 for module in self: --&gt; 100 input = module(input) 101 return input 102 /opt/conda/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) /opt/conda/envs/technical_blog/lib/python3.7/site-packages/dynamic_unet/encoder.py in forward(self, x) 51 out = self.relu(out) 52 &gt; 53 out = self.conv2(out) 54 out = self.bn2(out) 55 /opt/conda/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) /opt/conda/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/conv.py in forward(self, input) 343 344 def forward(self, input): --&gt; 345 return self.conv2d_forward(input, self.weight) 346 347 class Conv3d(_ConvNd): /opt/conda/envs/technical_blog/lib/python3.7/site-packages/torch/nn/modules/conv.py in conv2d_forward(self, input, weight) 340 _pair(0), self.dilation, self.groups) 341 return F.conv2d(input, weight, self.bias, self.stride, --&gt; 342 self.padding, self.dilation, self.groups) 343 344 def forward(self, input): KeyboardInterrupt: . And that&#39;s it! That&#39;s all you need to use pre-trained ResNets for image segmentation of natural images. . . Note: this post is a draft - I&#8217;m waiting on GPUs to do training using the Dice loss, to directly optimize for the Dice similarity metric. .",
            "url": "https://dthiagarajan.github.io/technical_blog/draft/pytorch/nbdev/2020/03/29/Using-NBDev-to-Package-Dynamic-UNet-Code.html",
            "relUrl": "/draft/pytorch/nbdev/2020/03/29/Using-NBDev-to-Package-Dynamic-UNet-Code.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Using PyTorch Hooks for the Dynamic U-Net Architecture [Draft]",
            "content": "Overview . In this notebook, we&#39;ll take a look in more detail about how to set up a segmentation network dynamically from a given ResNet backbone. Specifically, we&#39;ll take advantage of PyTorch hooks to setup the decoder layers for outputting a segmentation, in the scheme shown in the U-Net paper (image shown below). . The left half of the network will be referred to frequently as the encoder, and the right half of the network will be referred to frequently as the decoder. Briefly, the novel proposition when the U-Net paper was published was the idea of using skip connections (here, from the encoder to the decoder) to combat any loss of information when upsampling. . Concretely, by using skip connections that concatenate a level of encoder&#39;s output with the input to the corresponding level of the decoder, whenever an upsampling operation is done, there is 2 times the amount of information in the number of channels that is used for upsampling. . Encoder Setup . We&#39;ll first setup the code for the ResNet backbone (i.e. the encoder in the U-Net network, or the left-hand side). . #collapse_hide import matplotlib.pyplot as plt import numpy as np import os from PIL import Image import tifffile as tiff import torch import torch.nn as nn import torch.optim as optim from torch.nn import functional as F import torch.utils.model_zoo as model_zoo import torchvision.transforms.functional as tf from tqdm.notebook import tqdm . . The point of using the ResNet backbone is to leverage the pre-trained weights we can get from PyTorch&#39;s model zoo. The links are set up below, and will be used in our implementation for fetching the ResNet encoder backbone. . #collapse_hide model_urls = { &#39;resnet18&#39;: &#39;https://download.pytorch.org/models/resnet18-5c106cde.pth&#39;, &#39;resnet34&#39;: &#39;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#39;, &#39;resnet50&#39;: &#39;https://download.pytorch.org/models/resnet50-19c8e357.pth&#39;, &#39;resnet101&#39;: &#39;https://download.pytorch.org/models/resnet101-5d3b4d8f.pth&#39;, &#39;resnet152&#39;: &#39;https://download.pytorch.org/models/resnet152-b121ed2d.pth&#39;, } . . This code is taken from the official PyTorch implementations for ResNets. Some things are not ideal with how they initially trained these networks (based on recent empirical findings), but otherwise, these pre-trained weights are very useful as we now have a very good baseline to start with for segmentation of real images. . #collapse_hide def conv3x3(in_planes, out_planes, stride=1): &quot;&quot;&quot;3x3 convolution with padding&quot;&quot;&quot; return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False) def conv1x1(in_planes, out_planes, stride=1): &quot;&quot;&quot;1x1 convolution&quot;&quot;&quot; return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False) class BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out class Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None): super(Bottleneck, self).__init__() self.conv1 = conv1x1(inplanes, planes) self.bn1 = nn.BatchNorm2d(planes) self.conv2 = conv3x3(planes, planes, stride) self.bn2 = nn.BatchNorm2d(planes) self.conv3 = conv1x1(planes, planes * self.expansion) self.bn3 = nn.BatchNorm2d(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out . . The nice thing about ResNets are the residual blocks - these form very clear levels for the encoder and decoder of our U-Net to interface with. This is made even more clear in the implementation of the ResNet network - the variables self.layer1 to self.layer4 comprise these levels, and self.layer0 comprises the initial input encoding for the ResNet (generally going from the standard 3 channels of input from the image to 64 channels, i.e. 64 filters). . The implementation is modified slightly from the original PyTorch version. Specifically: . We don&#39;t need the last FC layer, as we&#39;ll have something different for connecting to the decoder | We encompass the input encoding for the network (self.conv1 to self.bn1 to self.relu) in self.layer0 for clarity in the forward function | . #collapse_show class ResNetEncoder(nn.Module): def __init__(self, block, layers, num_classes=1000): super(ResNetEncoder, self).__init__() self.inplanes = 64 self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer0 = nn.Sequential(self.conv1, self.bn1, self.relu) self.layer1 = nn.Sequential(self.maxpool, self._make_layer(block, 64, layers[0])) self.layer2 = self._make_layer(block, 128, layers[1], stride=2) self.layer3 = self._make_layer(block, 256, layers[2], stride=2) self.layer4 = self._make_layer(block, 512, layers[3], stride=2) self.out_dim = 512 * block.expansion for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;relu&#39;) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) def _make_layer(self, block, planes, blocks, stride=1): downsample = None if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), nn.BatchNorm2d(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample)) self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append(block(self.inplanes, planes)) return nn.Sequential(*layers) def forward(self, x): x = self.layer0(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) return x . . Now, we can load the pre-trained weights from the model_urls defined above. This is made easy using the utility API in PyTorch, model_zoo. Then, because we modified the ResNet implementation slightly, but kept the parameter names the same, we can load the state dictionary for any parameters still present (e.g. self.conv1, self.bn1, self.relu, and the self.layers) without being strict to load as much of the pre-trained weights as possible. . These function definitions (shown below) are again taken from the official PyTorch implementation, but the modification of making the weight loading non-strict, and (by default) loading a pre-trained network. . #collapse_show def resnet18(pretrained=True, **kwargs): &quot;&quot;&quot;Constructs a ResNet-18 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNetEncoder(BasicBlock, [2, 2, 2, 2], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet18&#39;]), strict=False) return model def resnet34(pretrained=True, **kwargs): &quot;&quot;&quot;Constructs a ResNet-34 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNetEncoder(BasicBlock, [3, 4, 6, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet34&#39;]), strict=False) return model def resnet50(pretrained=True, **kwargs): &quot;&quot;&quot;Constructs a ResNet-50 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNetEncoder(Bottleneck, [3, 4, 6, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet50&#39;]), strict=False) return model def resnet101(pretrained=True, **kwargs): &quot;&quot;&quot;Constructs a ResNet-101 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNetEncoder(Bottleneck, [3, 4, 23, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet101&#39;]), strict=False) return model def resnet152(pretrained=True, **kwargs): &quot;&quot;&quot;Constructs a ResNet-152 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNetEncoder(Bottleneck, [3, 8, 36, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet152&#39;]), strict=False) return model . . Decoder Setup . Now that we have our setup for the pre-trained ResNet encoder, we need to automatically construct the Decoder using the architecture given in the encoder. To do so, we&#39;ll define some helper layers as nn.Modules. . ConvLayer is just a general form of a convolution, ReLU, and batch normalization layer in sequence, with some empirical bets practices (e.g. initializing using $ frac{1}{ sqrt{5}}$ for all the weights in the convolutional layer, as per the FastAI course). | ConcatLayer is just a thin wrapper on the torch.cat function that concatenates all inputs along the channel dimension, assuming inputs are image batches, i.e. they have shape (batch size, num channels, height, width). | LambdaLayer is just a thin wrapper of a generic lambda function | upconv2x2 is a utility function for setting up convolutions that upsample an image. As mentioned above, in the U-Net architecture, we first concatenate the encoder output with the corresponding decoder input, so that when we upsample an image (i.e. from $(h, w)$ in size to $(2h, 2w)$ in size), we always have 2 times the amount of information (in this case, from having two times the number of channels). Accordingly, we will always convolve using an atrous convolution (where we dilate the kernel, rather than inserting 0s in the input to the convolutional layer), followed by the actual upsampling operation (using bilinear upsampling). | . class ConvLayer(nn.Module): def __init__(self, num_inputs, num_filters, bn=True, kernel_size=3, stride=1, padding=None, transpose=False, dilation=1): super(ConvLayer, self).__init__() if padding is None: padding = (kernel_size-1)//2 if transpose is not None else 0 if transpose: self.layer = nn.ConvTranspose2d(num_inputs, num_filters, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation) else: self.layer = nn.Conv2d(num_inputs, num_filters, kernel_size=kernel_size, stride=stride, padding=padding) nn.init.kaiming_uniform_(self.layer.weight, a=np.sqrt(5)) self.bn_layer = nn.BatchNorm2d(num_filters) if bn else None def forward(self, x): out = self.layer(x) out = F.relu(out) return out if self.bn_layer is None else self.bn_layer(out) class ConcatLayer(nn.Module): def forward(self, x, dim=1): return torch.cat(list(x.values()), dim=dim) class LambdaLayer(nn.Module): def __init__(self, f): super(LambdaLayer, self).__init__() self.f = f def forward(self, x): return self.f(x) def upconv2x2(inplanes, outplanes, size=None, stride=1): if size is not None: return [ ConvLayer(inplanes, outplanes, kernel_size=2, dilation=2, stride=stride), nn.Upsample(size=size, mode=&#39;bilinear&#39;, align_corners=True) ] else: return [ ConvLayer(inplanes, outplanes, kernel_size=2, dilation=2, stride=stride), nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;, align_corners=True) ] . Some specifics in how the decoder is coordinated (here, the first layer means the input encoding layer of the encoder, and the last layer indicates the last layer in the encoder). These details are not super important, and are probably understandable if you inspect the U-Net architecture image more closely. . The first layer&#39;s output passed along, concatenated, fed through conv3x3 before upsampling, then fed through a regular conv3x3 two times, then a conv1x1 to output the right number of channels for segmentation output | The middle layers output all are passed along, concatenated, and fed through a conv3x3 that first halves number of channels to upsample, then a regular conv3x3 | The last layer output&#39;s takes two pathways: Going down in the figure, the output goes through: max-pool (2x2), conv3x3, conv3x3, upconv2x2. These operations are encompassed in the DecoderConnect class. | Going across, assed across and concatenated to the result of above step | . | . Again, these details don&#39;t particularly matter, unless you&#39;re implementing the architecture yourself. The important point is that upsampling always happens after a concatenation of the encoder&#39;s output with the corresponding input to the corresponding level of the decoder. . #collapse_show class DecoderConnect(nn.Module): def __init__(self, inplanes, output_size): super(DecoderConnect, self).__init__() self.bottom_process = nn.Sequential( ConvLayer(inplanes, inplanes * 2, kernel_size=3), ConvLayer(inplanes * 2, inplanes * 2, kernel_size=3), *upconv2x2(inplanes * 2, inplanes, size=output_size) ) self.concat_process = nn.Sequential( ConcatLayer(), ConvLayer(inplanes * 2, inplanes * 2, kernel_size=1), ConvLayer(inplanes * 2, inplanes, kernel_size=3), ConvLayer(inplanes, inplanes, kernel_size=3) ) def forward(self, x): decoder_input = self.bottom_process(x) return self.concat_process({0: x, 1: decoder_input}) . . The crux of constructing the decoder happens in the setup_decoder function call below, and consequently in the construct_decoder. The details are hard to extract from the code below, so we can break it down as follows (tracing the code in the setup_decoder function first. . Getting Shapes Using Hooks . We&#39;re going to gather the input size and output size of a tensor to any layer in the ResNet encoder network with a name that has the prefix &quot;layer&quot;. To do so, we&#39;ll use hooks. Specifically, a hook is a closure, i.e. a function that&#39;s passed as an argument when registering a hook for a specific layer in our network. You can see here that . shape_hook is the function passed when registering a hook | We register_forward_hook for any child layer of our network that has a name that startswith layer, e.g. self.layer0, self.layer1, and so on. | . Note the specification for shape_hook, and generally for the function passed to register_forward_hook - it will have access to the input and output of the layer we are calling register_forward_hook for (note that input and output can be tuples here). In our case, we only care about their shapes, as we&#39;ll need the shape to determine the shape of the decoder&#39;s input, and accordingly the number of filters the convolutional layers need to output in the previous layer. . Accordingly, we&#39;ll take those shapes, and add them to our input_sizes and output_sizes array, to keep track of the input and output shapes as the network processes an input. To actually populate these arrays, we have to do exctly that - process an input. Thus, we&#39;ll make a dummy input (in the code, test_input) that we pass through our encoder, and after it finishes processing that input, our input_sizes and output_sizes array will be populated! . Constructing the Decoder . Now that we have the input and output sizes of any tensors passing through the blocks of our ResNet encoder, we can construct our decoder level by level. To do so, we&#39;ll just look at the following things: . How much we need to upsample the size of the image (determined by looking at the ratio of the input image size and the output image size) | What the difference in channels between the input and output of the corresponding encoder level are (determined by looking at the ratio of channels between input and output) | . Looking at both of these gives us a sense of the operation we need to do to reverse what the encoder did. Specifically, we can abide by the following assumptions when constructing the decoder: . The shape of the input to the level of the decoder we&#39;re working on will be the same as the shape of the output of the corresponding level of the encoder | The shape of the output of this level of the decoder will be the same shape as the shape of the input of the corresponding level of the encoder | . With these assumptions in mind, and using the details above for constructing the operations for each level of the decoder, we can just use case work for actually constructing the decoder, depending on whether we&#39;re looking at the last layer of the encoder, one of the middle layers, or the first layer of the encoder. . Since we&#39;re starting from the inputs and outputs of the first layer of the encoder, we add on the constructed layers as we inspect the shapes of the inputs and outputs of the encoder, and then reverse the list of constructed layers when finalizing the decoder architecture, to ensure that we go from the last output shape of the encoder to the first input shape of the encoder, which is (generally) what we want to output for segmentation. (This doesn&#39;t necessarily have to be true, in which case, a 1x1 convolution is added at the end of the decoder to get the right number of output channels, specified in the constructing of the class as num_output_channels. . Note that we maintain the decoder as a list of modules, i.e. an nn.ModuleList. This is an intentional choice, as we&#39;ll need to perform the operations of our network in sequence by level, as each level requires getting the corresponding output of the encoder, and processing it alongside the corresponding input of the decoder. . Model Forward Using Hooks . The last part of setting up our dynamic U-Net architecture is to specify the forward function. In order to do so, we need to keep track of the outputs of each level of our encoder. Since we&#39;ve encompassed the encoder as one module when constructing our U-Net, the easiest way to get the outputs for each level of the encoder is to just use hooks again. . The setup for these hooks is very similar to how we set up the shape hooks above, but instead, we only keep track of the outputs, and we want the actual output tensor, not the shape. This is encompassed in the encoder_output_hook hook in the forward function below. Again, we register the hook for all layers in our encoder that have name starting with &quot;layer&quot;. . To actually use these outputs, we only need to keep track of the corresponding input we are passing into the current level of the decoder. This becomes convenient to do since we left the decoder as an nn.ModuleList, so we need only iterate over the encoder outputs and the corresponding layer of the decoder that they&#39;ll be passed into with the corresponding input to the decoder. This is encompassed in the following loop in the forward function: . prev_output = None for reo, rdl in zip(reversed(encoder_outputs), self.decoder): if prev_output is not None: prev_output = rdl({0: reo, 1: prev_output}) else: prev_output = rdl(reo) . Note how that for the first layer of the decoder (the one that ties with the last layer of the encoder), there&#39;s no previous output. This is because the first layer of the decoder has the additional pathway (seen at the bottom of the U-Net architecture figure) that is concatenated with the output of the last layer of the encoder. On the other hand, for all other layers, the encoder output (reo) and the decoder input (prev_output) are concatenated together in a single pathway (explicitly, via the ConcatLayer forward function). . #collapse_show class DynamicUNet(nn.Module): def __init__(self, encoder, input_size=(224, 224), num_output_channels=None, verbose=0): super(DynamicUNet, self).__init__() self.encoder = encoder self.verbose = verbose self.input_size = input_size self.num_input_channels = 3 # This must be 3 because we&#39;re using a ResNet encoder self.num_output_channels = num_output_channels self.decoder = self.setup_decoder() def forward(self, x): encoder_outputs = [] def encoder_output_hook(self, input, output): encoder_outputs.append(output) handles = [ child.register_forward_hook(encoder_output_hook) for name, child in self.encoder.named_children() if name.startswith(&#39;layer&#39;) ] try: self.encoder(x) finally: if self.verbose &gt;= 1: print(&quot;Removing all forward handles&quot;) for handle in handles: handle.remove() prev_output = None for reo, rdl in zip(reversed(encoder_outputs), self.decoder): if prev_output is not None: prev_output = rdl({0: reo, 1: prev_output}) else: prev_output = rdl(reo) return prev_output def setup_decoder(self): input_sizes = [] output_sizes = [] def shape_hook(self, input, output): input_sizes.append(input[0].shape) output_sizes.append(output.shape) handles = [ child.register_forward_hook(shape_hook) for name, child in self.encoder.named_children() if name.startswith(&#39;layer&#39;) ] self.encoder.eval() test_input = torch.randn(1, self.num_input_channels, *self.input_size) try: self.encoder(test_input) finally: if self.verbose &gt;= 1: print(&quot;Removing all shape hook handles&quot;) for handle in handles: handle.remove() decoder = self.construct_decoder(input_sizes, output_sizes, num_output_channels=self.num_output_channels) return decoder def construct_decoder(self, input_sizes, output_sizes, num_output_channels=None): decoder_layers = [] for layer_index, (input_size, output_size) in enumerate(zip(input_sizes, output_sizes)): upsampling_size_factor = int(input_size[-1] / output_size[-1]) upsampling_channel_factor = input_size[-3] / output_size[-3] next_layer = [] bs, c, h, w = input_size ops = [] if layer_index == len(input_sizes) - 1: last_layer_ops = DecoderConnect(output_size[-3], output_size[2:]) last_layer_ops_input = torch.randn(*output_size) last_layer_concat_ops_output = last_layer_ops(last_layer_ops_input) next_layer.extend([last_layer_ops]) if upsampling_size_factor &gt; 1 or upsampling_channel_factor != 1: last_layer_concat_upconv_op = upconv2x2(output_size[-3], input_size[-3], size=input_size[2:]) last_layer_concat_upconv_op_output = nn.Sequential(*last_layer_concat_upconv_op)( last_layer_concat_ops_output ) next_layer.extend(last_layer_concat_upconv_op) elif layer_index == 0: first_layer_concat_ops = [ ConcatLayer(), ConvLayer(output_size[-3] * 2, output_size[-3] * 2, kernel_size=1), *upconv2x2( output_size[-3] * 2, output_size[-3], size=[dim * upsampling_size_factor for dim in output_size[2:]] ), ConvLayer(output_size[-3], output_size[-3], kernel_size=3), ConvLayer( output_size[-3], input_size[-3] if self.num_output_channels is None else self.num_output_channels, kernel_size=1 ), ] first_layer_concat_ops_output = nn.Sequential(*first_layer_concat_ops)( {0: torch.randn(*output_size), 1: torch.randn(*output_size)} ) next_layer.extend(first_layer_concat_ops) else: middle_layer_concat_ops = [ ConcatLayer(), ConvLayer(output_size[-3] * 2, output_size[-3] * 2, kernel_size=1), ConvLayer(output_size[-3] * 2, output_size[-3], kernel_size=3), ConvLayer(output_size[-3], output_size[-3], kernel_size=3) ] middle_layer_concat_ops_output = nn.Sequential(*middle_layer_concat_ops)( {0: torch.randn(*output_size), 1: torch.randn(*output_size)} ) next_layer.extend(middle_layer_concat_ops) if upsampling_size_factor &gt; 1 or upsampling_channel_factor != 1: middle_layer_concat_upconv_op = upconv2x2(output_size[-3], input_size[-3], size=input_size[2:]) middle_layer_concat_upconv_op_output = nn.Sequential(*middle_layer_concat_upconv_op)( middle_layer_concat_ops_output ) next_layer.extend(middle_layer_concat_upconv_op) decoder_layers.append(nn.Sequential(*next_layer)) return nn.ModuleList(reversed(decoder_layers)) . . Testing on the CamVid Dataset . For this example, we&#39;ll use the CamVid dataset, since they correspond to natural (3 channel) images, which is what our ResNet encoder expects. . #collapse_hide def load_camvid_dataset(data_directory): with open(os.path.join(data_directory, &quot;valid.txt&quot;), &quot;r&quot;) as f: val_names = [line.strip() for line in f] with open(os.path.join(data_directory, &quot;codes.txt&quot;), &quot;r&quot;) as f: label_mapping = {l.strip(): i for i, l in enumerate(f)} data = [] image_index_mapping = {} for im_f in os.listdir(os.path.join(data_directory, &quot;images&quot;)): if im_f.split(&#39;.&#39;)[-1] != &#39;png&#39;: continue image_index_mapping[im_f] = len(data) fp = os.path.join(data_directory, &quot;images&quot;, im_f) data.append(fp) for label_f in os.listdir(os.path.join(data_directory, &quot;labels&quot;)): im_f = label_f.split(&#39;.&#39;) im_f[0] = &#39;_&#39;.join(im_f[0].split(&#39;_&#39;)[:-1]) im_f = &#39;.&#39;.join(im_f) index = image_index_mapping[im_f] fp = os.path.join(data_directory, &quot;labels&quot;, label_f) data[index] = (data[index], fp) val_indices = [image_index_mapping[name] for name in val_names] return data, val_indices, label_mapping . . #collapse_hide camvid_data_directory = &quot;/home/jupyter/data/camvid&quot; all_data, val_indices, label_mapping = load_camvid_dataset(camvid_data_directory); . . We&#39;ll split the data for now into training and validation, based on the split specified in the dataset itself. . #collapse_show tr_data, val_data = [tpl for i, tpl in enumerate(all_data) if i not in val_indices], [tpl for i, tpl in enumerate(all_data) if i in val_indices] . . #collapse_hide def display_segmentation(image, target, ax=None): if ax: ax.imshow(image, cmap=&#39;gray&#39;) else: plt.imshow(image, cmap=&#39;gray&#39;) if ax: ax.imshow(target, cmap=&#39;jet&#39;, alpha=0.5) else: plt.imshow(target, cmap=&#39;jet&#39;, alpha=0.5) plt.show() def display_segmentation_from_file(im_f, label_f): im, label = Image.open(im_f), Image.open(label_f) display_segmentation(im, label) . . We can visualize some examples and their corresponding segmentations overlayed. . i = 10 display_segmentation_from_file(tr_data[i][0], tr_data[i][1]) . Here, we&#39;ll use a simple dataset class that transforms each image and it&#39;s corresponding segmentation label by randomly (with probably 0.5) vertically flipping, randomly (with probably 0.5) horizontally flipping, and then normalizing the input image using the standard ImageNet normalization statistics. . We also specify a resize shape, defaulting to 360 by 480, where nearest neighbor interpolation is used for resizing. . #collapse_show class CamvidDataset(torch.utils.data.Dataset): def __init__(self, data, resize_shape=(360, 480), is_train=True): self.images, self.labels = [tpl[0] for tpl in data], [tpl[1] for tpl in data] self.resize_shape = resize_shape self.is_train = is_train def transform(self, index): input, target = map( Image.open, (self.images[index], self.labels[index])) input, target = ( tf.resize(input, self.resize_shape), tf.resize(target, self.resize_shape, interpolation=Image.NEAREST) ) if self.is_train: horizontal_draw = torch.rand(1).item() vertical_draw = torch.rand(1).item() if horizontal_draw &gt; 0.5: input, target = tf.hflip(input), tf.hflip(target) if vertical_draw &gt; 0.5: input, target = tf.vflip(input), tf.vflip(target) input, target = map(tf.to_tensor, (input, target)) torch.clamp((255 * target), 0, 32, out=target) return tf.normalize(input, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), target.long() def __getitem__(self, index): return self.transform(index) def __len__(self): return len(self.images) . . tr_ds, val_ds = CamvidDataset(tr_data, resize_shape=(360, 480)), CamvidDataset(val_data, resize_shape=(360, 480), is_train=False) . tr_dl, val_dl = torch.utils.data.DataLoader(tr_ds, batch_size=4, shuffle=True), torch.utils.data.DataLoader(val_ds) . One last thing that will be useful is the Dice loss, used commonly in segmentation. It is implemented below. We use the differentiable variant (i.e. the smooth variant). In the literature, both the Dice loss and cross-entropy are used, with comparable results. For the purpose of this blog post, we&#39;ll optimize directly for the Dice score. . In paritcular, the reason for using the Dice score is that it is just the F1 score, which is the harmonic mean of the precision $P$ and the recall $R$. Specifically, we can write it as: . $$ frac{1}{ frac{1}{P} + frac{1}{R}}$$ . By using this loss, we optimize equally for precision and recall, where optimizing for cross-entropy is (theoretically) a bit more difficult due to background pixels being more likely to occur than specific class pixels. . class DiceLoss(nn.Module): &quot;&quot;&quot; Module to compute the Dice segmentation loss. Based on the following discussion: https://discuss.pytorch.org/t/one-hot-encoding-with-autograd-dice-loss/9781 &quot;&quot;&quot; def __init__(self, weights=None, ignore_index=None, eps=0.0001): super(DiceLoss, self).__init__() self.weights = weights self.ignore_index = ignore_index self.eps = eps def forward(self, output, target): &quot;&quot;&quot; Arguments: output: (N, C, H, W) tensor of probabilities for the predicted output target: (N, H, W) tensor corresponding to the pixel-wise labels Returns: loss: the Dice loss averaged over channels &quot;&quot;&quot; encoded_target = output.detach() * 0 if self.ignore_index is not None: mask = target == self.ignore_index target = target.clone() target[mask] = 0 encoded_target.scatter_(1, target.unsqueeze(1), 1) mask = mask.unsqueeze(1).expand_as(encoded_target) encoded_target[mask] = 0 else: encoded_target.scatter_(1, target.unsqueeze(1), 1) if self.weights is None: self.weights = 1 intersection = output * encoded_target numerator = 2 * intersection.sum(0).sum(1).sum(1) denominator = output + encoded_target if self.ignore_index is not None: denominator[mask] = 0 denominator = denominator.sum(0).sum(1).sum(1) + self.eps loss_per_channel = self.weights * (1 - (numerator / denominator)) return loss_per_channel.sum() / output.size(1) . def dice_similarity(output, target, weights=None, ignore_index=None, eps=1e-8): &quot;&quot;&quot; Arguments: output: (N, C, H, W) tensor of model output target: (N, H, W) tensor corresponding to the pixel-wise labels Returns: loss: the Dice loss averaged over channels &quot;&quot;&quot; prediction = torch.argmax(output, dim=1) encoded_prediction = output.detach() * 0 encoded_prediction.scatter_(1, prediction.unsqueeze(1), 1) encoded_target = output.detach() * 0 if ignore_index is not None: mask = target == ignore_index target = target.clone() target[mask] = 0 encoded_target.scatter_(1, target.unsqueeze(1), 1) mask = mask.unsqueeze(1).expand_as(encoded_target) encoded_target[mask] = 0 else: encoded_target.scatter_(1, target.unsqueeze(1), 1) if weights is None: weights = 1 intersection = encoded_prediction * encoded_target numerator = 2 * intersection.sum(0).sum(1).sum(1) + eps denominator = intersection + encoded_target if ignore_index is not None: denominator[mask] = 0 denominator = denominator.sum(0).sum(1).sum(1) + eps acc_per_channel = weights * ((numerator / denominator)) return acc_per_channel.sum() / output.size(1) . To begin, we&#39;ll only train the decoder of our model, and freeze the weights of our encoder (since they are pre-trained weights). This is easily done by only specifying the parameters of our decoder in the optimizer settings. . model = DynamicUNet(resnet34(), num_output_channels=32, input_size=(360, 480)) if torch.cuda.is_available(): model = model.cuda() decoder_parameters = [item for module in model.decoder for item in module.parameters()] optimizer = optim.AdamW(decoder_parameters) # Only training the decoder for now criterion = DiceLoss() # Training specific parameters num_epochs = 10 num_up_epochs, num_down_epochs = 3, 7 scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=1e-2, total_steps=num_epochs * len(tr_dl), ) . We&#39;ll train for 10 epochs to fine-tune the decoder to be on par with the pre-trained weights being used in the ResNet encoder. We&#39;re using the default learning rate for Adam, $10^{-3}$. We&#39;ll print the per-pixel accuracy, as well as the Dice similarity (which is the F1 score). . . Warning: The following cell runs training, and might take a while - take caution if you&#8217;re using a preemptible instance of a cloud service. . #collapse_hide model.train() losses = [] accuracies = [] tqdm_iterator = tqdm(range(num_epochs), position=0) for epoch in tqdm_iterator: tr_loss, tr_correct_pixels, tr_total_pixels, tr_dice_similarity, total = 0., 0., 0., 0., 0. tqdm_epoch_iterator = tqdm(tr_dl, position=1, leave=False) for i, (x, y) in enumerate(tqdm_epoch_iterator): optimizer.zero_grad() if torch.cuda.is_available(): x, y = x.cuda(), y.squeeze(dim=1).cuda() output = model(x) probs = torch.softmax(output, dim=1) prediction = torch.argmax(output, dim=1) tr_correct_pixels += ((prediction == y).sum()) tr_total_pixels += y.numel() tr_dice_similarity += dice_similarity(output, y.squeeze(1)) * len(y) loss = criterion(output, y.squeeze(1)) tr_loss += loss.data.cpu() * len(y) total += len(y) loss.backward() optimizer.step() if scheduler is not None: scheduler.step() if i % 1 == 0: curr_loss = tr_loss / total curr_acc = tr_correct_pixels / tr_total_pixels curr_dice = tr_dice_similarity / total tqdm_epoch_iterator.set_postfix({ &quot;Loss&quot;: curr_loss.item(), &quot;Accuracy&quot;: curr_acc.item(), &quot;Dice&quot;: curr_dice.item() }) overall_loss = tr_loss.item() / total overall_acc = tr_correct_pixels.item() / tr_total_pixels losses.append(overall_loss) accuracies.append(overall_acc) tqdm_iterator.set_postfix({&quot;Loss&quot;: overall_loss, &quot;Accuracy&quot;: overall_acc}) . . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-20-4caa77be1f34&gt; in &lt;module&gt; 12 if torch.cuda.is_available(): 13 x, y = x.cuda(), y.squeeze(dim=1).cuda() &gt; 14 output = model(x) 15 probs = torch.softmax(output, dim=1) 16 prediction = torch.argmax(output, dim=1) /opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) &lt;ipython-input-8-d9839ac87df8&gt; in forward(self, x) 32 for reo, rdl in zip(reversed(encoder_outputs), self.decoder): 33 if prev_output is not None: &gt; 34 prev_output = rdl({0: reo, 1: prev_output}) 35 else: 36 prev_output = rdl(reo) /opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) /opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py in forward(self, input) 98 def forward(self, input): 99 for module in self: --&gt; 100 input = module(input) 101 return input 102 /opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) &lt;ipython-input-6-cc6e2d84af2e&gt; in forward(self, x) 17 out = self.layer(x) 18 out = F.relu(out) &gt; 19 return out if self.bn_layer is None else self.bn_layer(out) 20 21 class ConcatLayer(nn.Module): /opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 530 result = self._slow_forward(*input, **kwargs) 531 else: --&gt; 532 result = self.forward(*input, **kwargs) 533 for hook in self._forward_hooks.values(): 534 hook_result = hook(self, input, result) /opt/conda/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py in forward(self, input) 105 input, self.running_mean, self.running_var, self.weight, self.bias, 106 self.training or not self.track_running_stats, --&gt; 107 exponential_average_factor, self.eps) 108 109 /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py in batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps) 1668 return torch.batch_norm( 1669 input, weight, bias, running_mean, running_var, -&gt; 1670 training, momentum, eps, torch.backends.cudnn.enabled 1671 ) 1672 KeyboardInterrupt: . Now we can take a look at the losses over time and the accuracies over time. . #collapse_hide plt.plot(list(range(len(losses))), losses) plt.title(&quot;Losses over time for fine-tuning decoder&quot;) plt.show() . . #collapse_hide plt.plot(list(range(len(accuracies))), accuracies) plt.title(&quot;Accuracies over time for fine-tuning decoder&quot;) plt.show() . . Let&#39;s save a model checkpoint so that, if necessary, we can reset back here if our next step goes badly. This is a good practice in general. . import pickle decoder_finetuned_model_state_dict = model.state_dict() . output_fp = &quot;/share/nikola/export/dt372/technical_blog/dynamic_unet/model_ckpts/decoder_finetuned_model_state_dict.pkl&quot; with open(output_fp, &#39;wb&#39;) as f: pickle.dump(decoder_finetuned_model_state_dict, f) . with open(output_fp, &#39;rb&#39;) as f: decoder_finetuned_model_state_dict = pickle.load(f) model.load_state_dict(decoder_finetuned_model_state_dict) . Now, we&#39;ll fine-tune the whole network. We&#39;ll use a smaller learning rate for the encoder, and a larger, but still small learning rate for the decoder. Ideally, we&#39;d do some sort of learning rate finding scheme (as done in FastAI), but for simplicity, we&#39;ll just pick reasonable numbers. . Now we&#39;ll train for 20 epochs, and see how the model performs over time. . optimizer = optim.AdamW([ {&#39;params&#39;: model.decoder.parameters(), &#39;lr&#39;: 1e-4}, {&#39;params&#39;: model.encoder.parameters(), &#39;lr&#39;: 1e-7}, ]) num_epochs = 20 num_up_epochs, num_down_epochs = 4, 16 scheduler = torch.optim.lr_scheduler.CyclicLR( optimizer, base_lr=[1e-6, 1e-8], max_lr=[1e-3, 1e-6], step_size_up=num_up_epochs * len(tr_dl), step_size_down=num_down_epochs * len(tr_dl), mode=&#39;exp_range&#39;, cycle_momentum=False # Need to set this since we&#39;re using AdamW? ) . #collapse_hide model.train() tqdm_iterator = tqdm(range(num_epochs), position=0) for epoch in tqdm_iterator: tr_loss, tr_correct_pixels, tr_total_pixels, total = 0., 0., 0., 0. tqdm_epoch_iterator = tqdm(tr_dl, position=1, leave=False) for i, (x, y) in enumerate(tqdm_epoch_iterator): optimizer.zero_grad() if torch.cuda.is_available(): x, y = x.cuda(), y.squeeze(dim=1).cuda() output = model(x) prediction = torch.argmax(output, dim=1) tr_correct_pixels += ((prediction == y).sum()) tr_total_pixels += y.numel() loss = criterion(output, y) tr_loss += loss.data.cpu() * len(y) total += len(y) loss.backward() optimizer.step() if scheduler is not None: scheduler.step() if i % 1 == 0: curr_loss = tr_loss / total curr_acc = tr_correct_pixels / tr_total_pixels tqdm_epoch_iterator.set_postfix({&quot;Loss&quot;: curr_loss.item(), &quot;Accuracy&quot;: curr_acc.item()}) overall_loss = tr_loss.item() / total overall_acc = tr_correct_pixels.item() / tr_total_pixels losses.append(overall_loss) accuracies.append(overall_acc) tqdm_iterator.set_postfix({&quot;Loss&quot;: overall_loss, &quot;Accuracy&quot;: overall_acc}) . . Again, let&#39;s look at how the losses and accuracies change (after epoch 10 is where we started the above fine-tuning cycle). . #collapse_hide plt.plot(list(range(len(losses))), losses) plt.title(&quot;Losses over time for fine-tuning decoder&quot;) plt.show() . . #collapse_hide plt.plot(list(range(len(accuracies))), accuracies) plt.title(&quot;Accuracies over time for fine-tuning decoder&quot;) plt.show() . . Now, we can visualize some results on the validation dataset. First is the prediction, second is the ground truth. . index = 10 x, y = val_dl.dataset[index] x, y = x.unsqueeze(0).cuda(), y.cuda() model.eval() output = model(x) prediction = torch.argmax(output, dim=1).cpu() display_segmentation(tf.to_pil_image(x[0].cpu()), tf.to_pil_image(prediction.byte())) display_segmentation_from_file(val_dl.dataset.images[index], val_dl.dataset.labels[index]) . Clearly, there&#39;s some room for improvement. . . Note: this post is a draft - I&#8217;m waiting on GPUs to do training using the Dice loss, to directly optimize for the Dice similarity metric. .",
            "url": "https://dthiagarajan.github.io/technical_blog/draft/pytorch/hooks/2020/03/18/Dynamic-UNet-and-PyTorch-Hooks.html",
            "relUrl": "/draft/pytorch/hooks/2020/03/18/Dynamic-UNet-and-PyTorch-Hooks.html",
            "date": " • Mar 18, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Essentials Guide to PyTorch Dataset and DataLoader Usage",
            "content": "Overview . In this short guide, we show a small representative example using the Dataset and DataLoader classes available in PyTorch for easy batching of training examples. This is more meant to be an onboarding for me with fastpages, but hopefully this example will be useful to those beginning to use PyTorch for their own applications. . Setup . The first thing we need is the essential import: torch, i.e. PyTorch. Make sure that when you&#39;re running a notebook with code similar to this that you&#39;ve imported torch, i.e. import torch, as shown below. . #collapse_hide import torch . . We&#39;ll then need a dataset to work with. For this small example, we&#39;ll use numpy to generate a random dataset for us. Specifically, we&#39;ll be working with a batch size of 32 later, so we&#39;ll create a dataset with exactly 50 batches, where each example has 5 features and a corresponding label between 0-9, inclusive. To do so, we use . np.random.randn for generating the input examples | np.random.randint for generating the labels | . The exact code is shown below. . #collapse_show import numpy as np training_examples = np.random.randn(32 * 50, 5) training_labels = np.random.randint(0, 10, size=(32*50,)) . . As a sanity check, let&#39;s look at the shapes. We&#39;ll want the size of the whole dataset to be (1600, 5), as we have $32*50$ examples, each with 5 features. Similarly, we&#39;ll want the size of the labels for the whole dataset to be (1600,), as we&#39;re essentially working with a list of 1600 labels. . #collapse_show training_examples.shape, training_labels.shape . . ((1600, 5), (1600,)) . We can look at some of the labels, just for a sanity check that they look reasonable. . #collapse_show training_labels[:10] . . array([8, 4, 3, 3, 9, 8, 1, 0, 5, 8]) . Dataset Class and Instantiation . Now, we&#39;ll create a simple PyTorch dataset class. All you need to implement within this class is the __getitem__ function and the __len__ function. . __getitem__ is a function that takes in an index, and returns dataset[index] | __len__ returns the size of your dataset (in this case, that&#39;s 32*50). | . When writing this class, you MUST subclass torch.utils.data.Dataset, as this is requirement for using the DataLoader class (see below). . class ExampleDataset(torch.utils.data.Dataset): &quot;&quot;&quot; You can define the __init__ function any way you like&quot;&quot;&quot; def __init__(self, examples, labels): self.examples = examples self.labels = labels &quot;&quot;&quot; This function signature always should take in 1 argument, corresponding to the index you&#39;re going to access. In this case, we&#39;re returning a tuple, corresponding to the training example and the corresponding label. It will also be useful to convert the returned values to torch.Tensors, so we can push the data onto the GPU later on. Note how the label is put into an array, but the example isn&#39;t. This is just a convention: if we don&#39;t put `self.labels[index]` in a list, it&#39;ll just create a tensor of zeros with `self.labels[index]` zeros. &quot;&quot;&quot; def __getitem__(self, index): return (torch.Tensor(self.examples[index]), torch.Tensor([self.labels[index]])) &quot;&quot;&quot; This function signature always should take in 0 arguments, and the output should be an `int`. &quot;&quot;&quot; def __len__(self): return len(self.examples) . Now, we can instantiate an instance of our ExampleDataset class, which subclasses torch.utils.data.Dataset. Note that we can specify how to initialize this via the __init__ function, which takes in a list of examples, and a list of labels (i.e. what we&#39;ve instantiated in our own setup). . training_dataset = ExampleDataset(training_examples, training_labels) . Sanity check - see the correspondence between accessing the dataset instance of the class above and the examples/labels we passed in. . training_dataset[0] . (tensor([ 0.4257, 0.0499, 0.2127, 0.8207, -1.1252]), tensor([8.])) . training_examples[0], training_labels[0] . (array([ 0.42565871, 0.04990118, 0.21268054, 0.82069534, -1.12520862]), 8) . We can iterate over this dataset using standard for loop syntax as well. The way you write the for loop depends on how __getitem__ is set up. In our case, we return a tuple (example and label), so the for loop should also have a tuple. . example, label = training_dataset[0] print(type(example), example.shape, type(label), label.shape) . &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([5]) &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([1]) . from tqdm import tqdm for example, label in tqdm(training_dataset): continue . 100%|██████████| 1600/1600 [00:00&lt;00:00, 85500.98it/s] . Batching via the DataLoader class . To set up batching, we&#39;ll use the torch.utils.data.DataLoader class. All we have to do to create this DataLoader is to instantiate it with our dataset we created above (training_dataset). The arguments for torch.utils.data.DataLoader are worth looking at, but (generally) most important are: . dataset: the PyTorch dataset class instance we&#39;ll pass in (e.g. training_dataset, this is why we had to do subclassing above) | batch_size (optional, default is 1): the batch size we want when iterating (we&#39;ll pass in 32) | shuffle (optional, default is False): whether we want to shuffle when iterating once the dataloader (note that if this is set to true, it&#39;ll shuffle every epoch; note also that we only really want to have this set to true for training, not necessarily for validation) | drop_last (optional, default is False): whether to drop the last incomplete batch (we don&#39;t have to worry about this because the number of training examples is exactly divisible by 32) | . training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True) . Again, we can iterate, just like we did for training_dataset, but now, we get batches back, as we can see by printing the shapes. The magic happens in the collate_fn optional argument of the DataLoader class, but the default behavior is sufficient here for batching the examples and labels separately. . We&#39;ll first ensure that there are exactly 50 batches in our dataloader to work with. . assert len(training_dataloader) == 50 . Now, mimicking the iteration from above, with the ExampleDataset instance: . for example, label in tqdm(training_dataloader): continue . 100%|██████████| 50/50 [00:00&lt;00:00, 2255.73it/s] . At some point, you may want to know information about a specific batch - accessing specific batches from the DataLoader is not as easy - I don&#39;t know of a way to grab a specific batch, other than doing something like the following. . training_dataloader_batches = [(example, label) for example, label in training_dataloader] . some_example, some_label = training_dataloader_batches[15] some_example.shape, some_label.shape . (torch.Size([32, 5]), torch.Size([32, 1])) . However, you can always access the underlying dataset by literally doing .dataset, as shown below. . training_dataloader.dataset . &lt;__main__.ExampleDataset at 0x7f2f03857510&gt; . training_dataloader.dataset[15] . (tensor([ 0.0763, -0.3285, -1.0820, 0.2322, -0.2848]), tensor([8.])) . GPU Usage . Using the GPU is also trivial, even with the batches from the dataloader. Ensure that you have the GPU runtime set first, then run the following. You can verify that GPU is available with the condition shown below before the loop. . if torch.cuda.is_available(): print(&quot;Using GPU.&quot;) . Using GPU. . for example, label in tqdm(training_dataloader): if torch.cuda.is_available(): example, label = example.cuda(), label.cuda() . 100%|██████████| 50/50 [00:03&lt;00:00, 15.79it/s] . Afterword and Resources . As mentioned above, it&#39;s useful to look at the documentation for torch.utils.data.DataLoader. Another way to do so within the notebook itself is to run the following within a cell of the notebook: . torch.utils.data.DataLoader? . There are many interesting things that you can do here, with the arguments allowed in the DataLoader. For example: . You may be working with an image dataset large enough that you don&#39;t want to open all the images (e.g. using PIL) before feeding them through your model. In that case, you can lazily open them by passing in a collate_fn that opens the images before collating the examples of a batch, since collate_fn is only called for each iteration when iterating over the DataLoader, and not when the DataLoader is instantiated. | You may not want to shuffle the dataset, as it might incur unnecessary computation. This is especially true if you have a separate DataLoader for your validation dataset, in which case there&#39;s no need to shuffle, as it won&#39;t affect the predictions. | If you have access to multiple CPUs on whatever machine you&#39;re working on, you can use num_workers to load batches ahead of time on the other CPUs, i.e. the other workers. | If you&#39;re working with a GPU, one of the more expensive steps in the pipeline is moving data from CPU to GPU - this can be sped up by using pin_memory, which ensures that the same space in the GPU RAM is used for the data being transferred from the CPU. | .",
            "url": "https://dthiagarajan.github.io/technical_blog/pytorch/2020/03/09/PyTorch-Dataset-and-DataLoaders.html",
            "relUrl": "/pytorch/2020/03/09/PyTorch-Dataset-and-DataLoaders.html",
            "date": " • Mar 9, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Welcome! My name is Dilip Thiagarajan - I’m an incoming AI Engineer at Paige, and was formerly a software engineer and intern at FB, on their Business Integrity ML, Computer Vision, and Ads Content Understanding teams (from most recent to least recent). . I hold a Masters of Engineering in CS, with a specialization in computer vision and machine learning, from Cornell University. I’m interested broadly in the application of computer vision in medical applications, as well as Bayesian machine learning, and statistical learning. . I’m also interested in more unconvential applications of machine learning, such as in sports - I’m fascinated with the derivation and usage of characteristic statistics in analyzing performance of NBA players. Ask me about ideas I have and things I’m working on for projects with respect to data from the NBA! . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://dthiagarajan.github.io/technical_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}